{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from utils.dataset import load_data_pbulk\n",
    "from algo.Hcformer_pretrain_1d import Hcformer # just change here for old algo version\n",
    "from algo.module import pearson_corr_coef\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "seed = 0\n",
    "num_workers = 8\n",
    "target_length = 240\n",
    "dim = 768\n",
    "depth = 11\n",
    "heads = 8\n",
    "output_heads = 1\n",
    "hic_1d_feat_num = 5\n",
    "\n",
    "add_hic_1d = False\n",
    "add_hic_2d = False\n",
    "batch_size = 32\n",
    "gpu = [3, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is [3, 7]\n"
     ]
    }
   ],
   "source": [
    "if len(gpu) > 0:\n",
    "    device = torch.device(f\"cuda:{gpu[0]}\")\n",
    "    print(f\"Device is {gpu}\")\n",
    "else:\n",
    "    device = torch.device(f\"cuda:{gpu}\" if (torch.cuda.is_available() and gpu >= 0) else \"cpu\")\n",
    "    print(f\"Device is {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_torch(coo_matrix: List[coo_matrix]):\n",
    "    dense_matrix = []\n",
    "    for m in coo_matrix:\n",
    "        m = m.toarray()\n",
    "        m = m + m.T\n",
    "        m /= torch.ones(400) + torch.eye(400)\n",
    "        dense_matrix.append(m)\n",
    "    return torch.stack(dense_matrix, dim=0)\n",
    "\n",
    "def evaluation(model, data_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(data_loader), dynamic_ncols=True) as t:\n",
    "            t.set_description('Evaluation: ')\n",
    "            total_pred = []\n",
    "            total_exp = []\n",
    "            for item in data_loader:\n",
    "                if add_hic_1d and add_hic_2d:\n",
    "                    seq, exp, hic_1d, hic_2d = item[0].to(device), item[1], item[2].to(device), item[3]\n",
    "                    hic_2d = sparse_to_torch(hic_2d).to(device)\n",
    "                elif add_hic_1d:\n",
    "                    seq, exp, hic_1d = item[0].to(device), item[1], item[2].to(device)\n",
    "                    hic_2d = None\n",
    "                elif add_hic_2d:\n",
    "                    seq, exp, hic_2d = item[0].to(device), item[1], item[2]\n",
    "                    hic_1d = None\n",
    "                    hic_2d = sparse_to_torch(hic_2d).to(device)\n",
    "                else:\n",
    "                    seq, exp = item[0].to(device), item[1].to(device)\n",
    "                    hic_1d, hic_2d = None, None\n",
    "                pred = model(seq, head='human', hic_1d=hic_1d, hic_2d=hic_2d)\n",
    "\n",
    "                total_pred.append(pred.detach().cpu())\n",
    "                total_exp.append(exp.unsqueeze(-1))\n",
    "                t.update()\n",
    "            total_pred = torch.concat(total_pred, dim=0)\n",
    "            total_exp  = torch.concat(total_exp,  dim=0)\n",
    "\n",
    "    return pearson_corr_coef(total_pred, total_exp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_loader = load_data_pbulk(\n",
    "    path = data_path, \n",
    "    seed = seed, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = num_workers, \n",
    "    target_len = target_length,\n",
    "    hic_1d = add_hic_1d,\n",
    "    hic_2d = add_hic_2d)\n",
    "\n",
    "model = Hcformer.from_hparams(\n",
    "    dim = dim,\n",
    "    seq_dim = dim,\n",
    "    depth = depth,\n",
    "    heads = heads,\n",
    "    output_heads = dict(human=output_heads),\n",
    "    target_length = target_length,\n",
    "    dim_divisible_by = dim / 12,\n",
    "    hic_1d = add_hic_1d,\n",
    "    hic_1d_feat_num = hic_1d_feat_num,       \n",
    "    hic_1d_feat_dim = dim,\n",
    "    hic_2d = add_hic_2d,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(gpu) > 1:\n",
    "    model = nn.DataParallel(model, device_ids=gpu)\n",
    "\n",
    "model.load_state_dict(torch.load('/home/han_harry_zhang/SeqHiC2RNA/output/hcformer_pbulk/model/hcformer_pbulk5/yakuks2o'))\n",
    "\n",
    "# model.load_state_dict(torch.load('/home/han_harry_zhang/SeqHiC2RNA/output/hcformer_pbulk/model/hcformer_pbulk2/ai9o5a06'))\n",
    "# if len(gpu) > 1:\n",
    "#     model = nn.DataParallel(model, device_ids=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: : 100%|██████████| 141/141 [01:17<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_loader), dynamic_ncols=True) as t:\n",
    "        t.set_description('Evaluation: ')\n",
    "        total_pred = []\n",
    "        total_exp = []\n",
    "        for item in test_loader:\n",
    "            if add_hic_1d and add_hic_2d:\n",
    "                seq, exp, hic_1d, hic_2d = item[0].to(device), item[1], item[2].to(device), item[3]\n",
    "                hic_2d = sparse_to_torch(hic_2d).to(device)\n",
    "            elif add_hic_1d:\n",
    "                seq, exp, hic_1d = item[0].to(device), item[1], item[2].to(device)\n",
    "                hic_2d = None\n",
    "            elif add_hic_2d:\n",
    "                seq, exp, hic_2d = item[0].to(device), item[1], item[2]\n",
    "                hic_1d = None\n",
    "                hic_2d = sparse_to_torch(hic_2d).to(device)\n",
    "            else:\n",
    "                seq, exp = item[0].to(device), item[1]\n",
    "                hic_1d, hic_2d = None, None\n",
    "            pred = model(seq, head='human', hic_1d=hic_1d, hic_2d=hic_2d)\n",
    "\n",
    "            total_pred.append(pred.detach().cpu())\n",
    "            total_exp.append(exp.unsqueeze(-1))\n",
    "            t.update()\n",
    "        total_pred = torch.concat(total_pred, dim=0)\n",
    "        total_exp  = torch.concat(total_exp,  dim=0)\n",
    "mean_test_pearson_corr_coef = pearson_corr_coef(total_pred, total_exp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_center = total_pred - total_pred.mean(dim = 1, keepdim = True)\n",
    "exp_center = total_exp - total_exp.mean(dim = 1, keepdim = True)\n",
    "test_pearson_list = F.cosine_similarity(pred_center, exp_center, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3726, 0.2786, 0.3304, 0.3433, 0.4711, 0.3078])\n",
      "tensor([0.2439, 0.2238, 0.2341, 0.2375, 0.2716, 0.2155])\n",
      "tensor([ 2.7672e-01,  4.6281e-01,  1.2892e-01,  4.6896e-01,  3.7157e-01,\n",
      "         5.0322e-01,  3.6859e-02,  2.5045e-03,  2.5428e-01,  3.9096e-01,\n",
      "         4.8517e-01,  6.6358e-01,  5.0590e-01,  4.6288e-01,  3.9558e-01,\n",
      "         1.9238e-01,  2.5985e-01,  5.4446e-01,  6.4720e-01,  4.7370e-01,\n",
      "         5.2818e-03,  4.1119e-01,  3.6931e-01,  1.9615e-01,  3.3771e-01,\n",
      "         8.9374e-02,  1.1863e-02,  3.9876e-01, -1.9272e-02,  7.5622e-02,\n",
      "         5.5938e-01,  4.7661e-01,  2.2251e-01,  3.0453e-01,  5.4599e-01,\n",
      "         5.2379e-01,  5.2165e-01,  1.3492e-01, -9.4183e-05,  4.3120e-01,\n",
      "         5.5438e-01,  4.3887e-01,  5.3409e-01,  5.0853e-01,  5.2469e-01,\n",
      "         4.6138e-01,  3.5571e-02,  2.2146e-01,  3.2400e-01,  8.6585e-03,\n",
      "         5.4530e-01,  6.9646e-01,  1.4004e-01,  7.0693e-01,  3.8285e-01,\n",
      "         4.4742e-01,  6.7590e-01,  7.6496e-01,  5.2466e-01,  4.7534e-01,\n",
      "         5.3973e-01,  1.9153e-01,  1.5792e-01,  1.6609e-01,  5.8406e-01,\n",
      "         1.3208e-01,  5.3441e-01,  1.5213e-01,  5.9979e-01,  5.4449e-01,\n",
      "         2.0063e-01,  7.8791e-01,  1.4817e-01,  5.5278e-01,  4.4373e-01,\n",
      "         2.4047e-01,  5.2232e-01,  4.0802e-01,  1.5604e-01,  3.8688e-01,\n",
      "         6.2654e-01,  2.1054e-01,  4.5456e-01,  2.0251e-01,  6.0690e-01,\n",
      "         5.5597e-01,  3.9066e-01,  3.7793e-01,  5.1020e-01,  6.4264e-01,\n",
      "         3.4368e-01,  3.6256e-01, -4.0263e-02,  5.3563e-01,  4.8376e-01,\n",
      "         4.6140e-01,  6.2619e-01,  4.6235e-01,  5.0834e-01,  1.0353e-01,\n",
      "         4.5211e-01,  4.6352e-01,  5.5479e-01,  5.5153e-01,  4.7570e-01,\n",
      "         4.5320e-01,  2.3840e-01,  4.2528e-01,  3.2534e-01,  1.3880e-01,\n",
      "         3.6289e-01,  7.3689e-01,  1.7482e-01,  5.4617e-01,  3.9986e-01,\n",
      "         4.0793e-01,  4.6084e-01,  4.8507e-01,  5.8685e-01,  3.1911e-01,\n",
      "         5.4989e-01,  5.8294e-01,  6.5776e-01,  5.7411e-01,  2.3622e-01,\n",
      "         5.3245e-01,  3.2173e-01,  4.4686e-01,  4.6969e-01,  1.6515e-01,\n",
      "         1.3521e-01,  5.3015e-01,  1.7126e-01,  1.1640e-01,  1.0199e-01,\n",
      "         7.4624e-01,  5.4506e-01,  4.1796e-01,  4.0886e-01,  5.9227e-01,\n",
      "         4.1472e-01,  2.5845e-01,  6.4990e-01,  3.5516e-01,  1.0613e-01,\n",
      "         6.0195e-01,  1.9150e-01,  2.9629e-01,  5.3942e-01,  4.1043e-01,\n",
      "         3.2384e-02,  3.2122e-02,  3.6653e-01,  1.4964e-01,  1.5111e-01,\n",
      "         6.9747e-01,  2.8064e-01,  3.7283e-02,  4.8016e-01, -2.3710e-02,\n",
      "         5.4041e-01,  3.5892e-01,  3.9674e-01,  3.3360e-01,  2.8614e-01,\n",
      "         1.2216e-01,  5.6020e-01,  1.9087e-01,  3.8576e-01,  3.6983e-01,\n",
      "         4.2745e-01,  3.5681e-01,  4.7262e-01,  4.9253e-01,  5.6131e-01,\n",
      "         8.3923e-01,  1.8563e-02,  4.3319e-01,  5.3127e-01,  4.5625e-01,\n",
      "         1.5454e-01,  5.3536e-01,  6.1416e-01,  4.2048e-01,  5.0806e-01,\n",
      "         3.6327e-01,  3.3327e-01,  2.5570e-01,  4.0491e-01,  1.6813e-01,\n",
      "         6.7955e-01,  2.5158e-01,  1.4907e-01,  2.8937e-01,  5.7550e-01,\n",
      "         1.6983e-01,  2.8071e-01,  3.7961e-01,  6.6209e-01,  1.9846e-01,\n",
      "         1.5849e-01,  1.7198e-01,  5.1496e-01,  5.5765e-01,  6.8478e-01,\n",
      "         4.1695e-01,  4.6534e-01,  5.0167e-02,  4.8466e-01,  3.4957e-01,\n",
      "         4.1790e-01,  1.5502e-01,  1.2043e-01,  4.0237e-01,  4.1747e-01,\n",
      "         2.5568e-01,  3.0803e-01,  4.9273e-01,  1.8551e-01,  4.6729e-01,\n",
      "         3.9006e-01,  3.7829e-01,  4.0794e-01,  3.2103e-01,  8.2882e-02,\n",
      "         5.6019e-01,  3.6241e-01,  4.5308e-01,  5.0941e-01,  7.8048e-02,\n",
      "         4.9677e-01,  4.9018e-01,  4.8933e-01,  2.6658e-01,  1.8976e-01,\n",
      "         1.1421e-01,  5.9284e-01,  4.2975e-01,  4.9229e-01,  3.8825e-01,\n",
      "         6.0869e-02,  4.2235e-01,  1.2344e-01,  3.4662e-01,  3.6165e-01,\n",
      "         1.9107e-01,  5.2978e-01,  4.2429e-01,  3.3767e-01,  2.1188e-01,\n",
      "         3.1197e-01,  4.0973e-01,  4.1252e-01,  2.9267e-01,  4.6059e-01,\n",
      "         3.6889e-01,  3.3673e-01,  5.9068e-01,  1.9323e-01,  6.0205e-01,\n",
      "         6.1436e-01,  2.5833e-01,  4.5989e-01,  4.1592e-01,  4.0604e-03,\n",
      "         7.1937e-01,  3.2672e-01,  4.7910e-01,  4.5495e-01,  4.6482e-01,\n",
      "         6.6173e-01,  4.7021e-01,  4.9864e-01,  1.5010e-01,  9.6416e-02,\n",
      "         3.0921e-01, -6.7584e-03,  4.7149e-01,  4.8241e-01,  3.1403e-01,\n",
      "         4.7267e-01,  4.7287e-01,  4.8137e-01,  6.9500e-01,  2.7264e-01,\n",
      "         5.3473e-01,  2.3677e-01,  4.4055e-01,  2.5503e-01,  2.9030e-01,\n",
      "         6.6699e-01,  5.2672e-01,  5.2894e-01,  4.4603e-01,  2.7632e-01,\n",
      "         4.1251e-01,  2.8651e-01,  5.4089e-01,  3.4033e-01,  4.7064e-01,\n",
      "         2.5293e-01,  5.6554e-01,  3.6302e-01,  6.5527e-02,  5.6461e-01,\n",
      "         3.9713e-01,  2.2912e-01,  2.7858e-01,  6.4761e-01,  8.8750e-01,\n",
      "         5.0946e-01,  5.9256e-01,  2.9462e-01,  5.9438e-01,  2.8629e-01,\n",
      "         6.2207e-01,  6.2517e-01,  3.8999e-01,  5.3341e-01, -5.8516e-03,\n",
      "         6.1754e-01,  4.0493e-01,  5.1612e-01,  2.8937e-01,  1.6068e-02,\n",
      "         4.7692e-01,  4.7523e-01,  2.4691e-01,  6.0475e-01,  5.3198e-01,\n",
      "         3.6078e-01,  6.0785e-01,  3.7330e-01,  3.9806e-01,  2.5638e-01,\n",
      "         2.8489e-01,  3.0192e-01,  4.7796e-01,  1.5265e-01,  1.8562e-01,\n",
      "         4.1642e-01,  3.5930e-01,  4.8675e-01,  6.5661e-01,  5.3996e-01,\n",
      "         4.7546e-01,  1.8868e-01,  3.1489e-01, -2.4681e-02,  6.3801e-02,\n",
      "         4.4499e-01,  6.4695e-02,  3.6905e-01,  4.8550e-01,  4.5593e-01,\n",
      "         4.0544e-01,  7.2775e-02,  1.7347e-01,  1.8412e-01,  4.7558e-01,\n",
      "         5.3960e-01,  5.0576e-01,  2.4203e-01,  5.6197e-01, -1.3168e-02,\n",
      "         3.4572e-01,  3.2911e-01,  6.1246e-01,  5.3575e-01,  2.0848e-01,\n",
      "         5.0498e-02,  3.0159e-01,  1.9759e-01,  5.8695e-01,  7.4247e-01,\n",
      "         4.8228e-01,  6.4476e-01,  3.1096e-01, -5.5606e-06,  2.3343e-01,\n",
      "         7.3683e-02,  1.0087e-02,  4.6564e-01,  8.4412e-02,  1.8440e-01,\n",
      "         5.9819e-01,  1.4815e-02,  3.2580e-01,  8.0551e-03,  4.1165e-01,\n",
      "         5.1503e-01,  5.1786e-01,  2.7962e-01,  4.0302e-01,  5.4053e-01,\n",
      "         6.2966e-01,  1.2475e-01,  5.7228e-01,  2.3825e-01, -1.2690e-02,\n",
      "         5.2316e-01,  6.0484e-02,  5.8475e-01,  1.9291e-01,  4.0670e-01,\n",
      "         2.2562e-01,  3.1754e-02,  5.9439e-01,  4.8871e-01,  5.2515e-01,\n",
      "         5.7423e-01,  6.4160e-01,  4.9450e-01,  1.6283e-01,  4.7466e-01,\n",
      "         1.1509e-01,  5.7825e-01, -9.7476e-03,  2.9005e-01,  5.1711e-01,\n",
      "         4.8836e-01,  5.3383e-01,  6.9426e-01,  5.6388e-02,  7.5222e-02,\n",
      "         3.3462e-01,  5.7077e-01, -1.2723e-02,  2.8874e-02,  4.6725e-01,\n",
      "         3.6956e-02,  4.7932e-01, -1.0962e-02,  5.9182e-01,  9.0122e-02,\n",
      "         4.9073e-01, -3.1328e-02,  1.0558e-01, -5.4062e-02,  4.5406e-01,\n",
      "         2.5871e-01,  4.6672e-02,  5.3876e-01,  1.0862e-01,  1.6885e-01,\n",
      "         4.5101e-01,  4.8130e-01,  4.5943e-01,  1.2782e-01,  2.1865e-01,\n",
      "         1.1764e-01,  3.7688e-01,  1.9543e-02,  8.0842e-02,  2.0727e-01,\n",
      "         1.3051e-01,  5.8604e-01,  1.8499e-02,  3.6354e-01,  4.9685e-01,\n",
      "         3.3025e-01,  1.1931e-01,  5.2614e-01,  2.2481e-01,  2.9363e-01,\n",
      "        -3.9267e-02,  2.2642e-01,  4.8400e-01, -2.6484e-02,  4.6030e-01,\n",
      "         2.2754e-01,  5.7937e-01,  4.1480e-01,  1.5215e-01,  5.9489e-01,\n",
      "         3.7629e-01,  3.9357e-01,  6.8138e-03,  5.4042e-01,  9.3575e-01,\n",
      "         4.4646e-01,  3.3193e-01,  5.9681e-01,  4.9754e-01,  4.3939e-01,\n",
      "         3.9649e-01,  5.9224e-01,  2.5619e-01,  6.4937e-01,  3.2726e-01,\n",
      "         1.4534e-01,  6.5439e-01,  1.2018e-02,  6.8780e-01,  2.3604e-01,\n",
      "         1.9357e-01,  4.1849e-01,  4.3544e-01,  4.3533e-01,  2.4871e-02,\n",
      "         4.2361e-01,  2.1280e-01,  9.7465e-02,  4.3639e-01,  6.3984e-01,\n",
      "         6.7423e-01,  4.8081e-01,  8.5737e-02,  2.5902e-01,  3.2027e-01,\n",
      "         7.2296e-01,  6.8050e-01,  6.5545e-01,  1.2568e-01,  5.1310e-01,\n",
      "        -6.6951e-03,  2.9456e-01,  5.0253e-01,  2.2858e-01,  5.5474e-01,\n",
      "         5.3116e-01,  1.9795e-01,  5.2607e-01,  5.2609e-01,  3.1929e-02,\n",
      "         6.4652e-01,  5.5876e-01,  3.3065e-01,  9.6969e-02,  2.7713e-01,\n",
      "         1.5927e-01,  2.6406e-01,  5.8710e-01,  2.4162e-01,  2.1836e-01,\n",
      "         9.9735e-02,  5.3858e-01,  2.8432e-02,  5.4452e-01,  4.7398e-01,\n",
      "         3.4534e-01,  1.9279e-01,  4.0501e-01,  4.2771e-01,  6.1039e-01,\n",
      "         1.2556e-01,  2.5076e-01,  2.2384e-01,  2.9620e-01,  2.2420e-01,\n",
      "         7.5117e-02,  5.1038e-01,  4.3765e-01,  1.1345e-01,  5.6487e-01,\n",
      "         1.5476e-01,  7.4265e-01,  6.5233e-02,  6.2509e-01,  5.0210e-01,\n",
      "         4.1138e-01,  3.5530e-01,  1.4713e-02,  3.8089e-02,  7.0378e-01,\n",
      "         3.6941e-02,  9.6286e-02,  1.5938e-01,  1.9004e-01,  3.8230e-01,\n",
      "         1.3808e-01,  9.0378e-03, -2.3179e-02,  1.1594e-01,  1.2528e-01,\n",
      "         2.7767e-01,  4.8295e-01,  1.4715e-02,  4.0869e-01,  2.7995e-01,\n",
      "         2.1495e-01,  2.4011e-02,  6.4813e-01,  3.6572e-01,  5.4118e-01,\n",
      "         4.6413e-01,  4.2333e-01,  4.1297e-01,  3.4019e-01, -1.1094e-02,\n",
      "         3.9163e-01,  3.6884e-01,  4.9970e-01,  1.6162e-01,  2.9764e-01,\n",
      "         1.9359e-01,  4.6985e-01,  3.4134e-02,  3.4394e-01,  3.4323e-01,\n",
      "         9.6250e-03,  4.1836e-01,  3.2505e-01,  6.6067e-01,  3.7458e-01,\n",
      "         6.2052e-01,  6.6747e-02,  4.4740e-01,  5.1297e-01,  4.9320e-02,\n",
      "         2.1247e-01, -1.8090e-02,  5.8501e-01,  4.3412e-01,  6.1652e-01,\n",
      "         3.6617e-01,  4.3543e-01,  5.4660e-01,  4.3497e-01,  4.0620e-01,\n",
      "         5.5245e-01,  1.6783e-01,  4.6728e-01,  8.8372e-02,  4.9551e-01,\n",
      "         3.8845e-01,  2.1388e-01,  3.3042e-01,  1.7055e-01,  4.4039e-02,\n",
      "         1.4260e-01,  6.0217e-01,  3.2219e-01,  4.7600e-02,  4.0509e-01,\n",
      "         4.3432e-01,  5.5429e-01,  3.4210e-02,  6.1176e-01,  3.4820e-01,\n",
      "         3.5131e-02,  3.6739e-01,  7.1805e-01,  2.5044e-01,  5.5475e-01,\n",
      "         3.5620e-01,  1.0137e-01,  5.1766e-01,  3.5640e-02,  4.3205e-01,\n",
      "        -2.7614e-03,  5.1967e-01,  2.3917e-01,  7.5146e-01, -1.8128e-02,\n",
      "         9.2709e-02, -2.4569e-03,  4.7269e-02,  2.3380e-01,  1.1314e-01,\n",
      "         1.6381e-01, -3.6134e-02,  4.2832e-01,  4.7922e-01,  4.9880e-01,\n",
      "         2.9305e-02,  4.0739e-01,  1.2857e-01,  3.2887e-02,  2.5546e-02,\n",
      "         3.6647e-01,  2.8448e-01,  4.0140e-02,  1.0332e-01,  4.6620e-01,\n",
      "         3.0957e-01,  4.4304e-01,  1.4795e-01,  4.2041e-01,  6.4216e-01,\n",
      "         5.0396e-01,  9.4745e-02,  2.5734e-02,  2.2092e-01,  1.4482e-01,\n",
      "         5.8732e-01,  2.5174e-02,  2.6870e-01,  2.0339e-01,  5.9083e-02,\n",
      "         3.5321e-01,  2.0140e-01,  5.5742e-01,  8.9029e-02,  1.9184e-02,\n",
      "         3.1583e-01,  5.5589e-01,  1.1712e-01,  6.3237e-01,  4.1738e-01,\n",
      "         6.3828e-02,  3.9636e-02,  3.3656e-01,  2.0124e-02,  4.3516e-01,\n",
      "         5.9453e-01, -4.5822e-03,  4.5897e-01,  1.3131e-01,  7.2509e-01,\n",
      "         3.7474e-01,  7.2566e-01,  4.5587e-02,  3.8329e-01,  7.0830e-01,\n",
      "         4.9817e-01,  2.9330e-01,  6.8340e-01,  6.8934e-01,  3.4413e-01,\n",
      "         5.0742e-01,  2.7741e-01,  1.3246e-02,  4.7084e-01,  1.8097e-01,\n",
      "         5.6779e-01,  3.2266e-02,  5.5164e-02,  2.6050e-01,  5.5149e-01,\n",
      "         5.2011e-01,  1.9433e-02,  6.0949e-01,  7.0444e-02, -3.9325e-02,\n",
      "         3.3743e-01, -2.1093e-03,  4.2555e-01,  5.2826e-01,  3.1802e-01,\n",
      "         5.8975e-01,  5.9991e-01,  7.3840e-02,  4.6956e-02,  4.0222e-01,\n",
      "         7.0827e-01,  3.5500e-01,  5.3828e-01])\n",
      "tensor([0.1151, 0.1623, 0.1416, 0.1455, 0.1422, 0.1386, 0.0632, 0.0061, 0.1202,\n",
      "        0.1540, 0.1597, 0.1547, 0.1074, 0.2515, 0.0856, 0.1715, 0.0835, 0.1026,\n",
      "        0.0965, 0.1661, 0.0392, 0.0948, 0.0972, 0.1349, 0.2415, 0.0556, 0.0278,\n",
      "        0.1638, 0.0266, 0.1409, 0.0856, 0.1051, 0.1306, 0.0732, 0.1336, 0.1384,\n",
      "        0.1240, 0.1801, 0.0370, 0.1470, 0.1588, 0.1933, 0.0811, 0.2134, 0.1527,\n",
      "        0.1785, 0.1626, 0.1310, 0.0880, 0.0973, 0.1772, 0.1136, 0.0357, 0.1526,\n",
      "        0.1192, 0.1332, 0.1110, 0.1151, 0.1659, 0.2005, 0.1145, 0.2283, 0.2068,\n",
      "        0.1177, 0.1175, 0.1951, 0.1009, 0.1170, 0.1230, 0.1826, 0.1481, 0.1347,\n",
      "        0.1630, 0.1476, 0.1630, 0.1408, 0.0744, 0.1086, 0.1658, 0.2416, 0.1688,\n",
      "        0.1517, 0.0804, 0.1140, 0.0651, 0.0948, 0.0706, 0.1274, 0.1549, 0.1267,\n",
      "        0.1348, 0.1102, 0.1190, 0.1076, 0.2257, 0.1416, 0.1741, 0.1219, 0.1210,\n",
      "        0.1168, 0.1626, 0.1131, 0.1987, 0.1123, 0.1419, 0.0845, 0.1319, 0.1172,\n",
      "        0.1900, 0.1020, 0.1221, 0.1185, 0.1222, 0.1608, 0.0983, 0.1297, 0.1242,\n",
      "        0.0429, 0.1819, 0.1840, 0.1267, 0.1660, 0.0787, 0.1333, 0.1340, 0.1449,\n",
      "        0.2050, 0.1282, 0.2104, 0.1660, 0.1277, 0.0765, 0.1221, 0.1530, 0.0905,\n",
      "        0.0781, 0.1736, 0.1034, 0.2214, 0.0761, 0.2069, 0.0212, 0.1283, 0.0759,\n",
      "        0.1672, 0.1729, 0.1138, 0.1002, 0.2107, 0.2131, 0.0511, 0.0867, 0.1361,\n",
      "        0.2085, 0.1213, 0.1353, 0.1312, 0.0407, 0.1341, 0.0905, 0.1369, 0.1491,\n",
      "        0.0915, 0.1667, 0.2326, 0.1849, 0.0880, 0.1413, 0.1863, 0.1124, 0.0911,\n",
      "        0.0979, 0.1312, 0.0968, 0.2320, 0.0473, 0.0595, 0.1279, 0.1539, 0.1779,\n",
      "        0.2587, 0.0970, 0.1034, 0.1371, 0.1439, 0.1395, 0.1376, 0.1260, 0.1739,\n",
      "        0.1560, 0.0727, 0.1864, 0.1511, 0.2394, 0.1140, 0.1722, 0.1062, 0.2542,\n",
      "        0.0922, 0.1214, 0.1607, 0.0794, 0.2060, 0.1581, 0.1366, 0.0853, 0.1077,\n",
      "        0.0798, 0.1197, 0.1220, 0.1839, 0.1336, 0.2041, 0.1585, 0.1598, 0.1280,\n",
      "        0.1068, 0.1571, 0.2241, 0.3300, 0.2292, 0.1211, 0.1015, 0.2525, 0.1213,\n",
      "        0.2732, 0.1579, 0.0499, 0.1698, 0.0922, 0.1275, 0.1747, 0.1335, 0.1914,\n",
      "        0.2123, 0.1001, 0.0873, 0.0822, 0.2664, 0.1549, 0.3312, 0.2684, 0.1009,\n",
      "        0.1449, 0.0984, 0.1397, 0.1763, 0.1447, 0.0790, 0.1107, 0.2011, 0.1318,\n",
      "        0.1416, 0.1001, 0.1157, 0.1009, 0.1393, 0.0750, 0.1235, 0.1295, 0.0818,\n",
      "        0.0573, 0.0599, 0.1636, 0.0244, 0.0881, 0.0723, 0.1432, 0.1169, 0.1480,\n",
      "        0.1086, 0.1481, 0.1856, 0.1470, 0.1169, 0.0771, 0.0172, 0.1011, 0.1097,\n",
      "        0.1060, 0.1585, 0.1215, 0.2143, 0.0802, 0.1077, 0.1257, 0.2738, 0.1537,\n",
      "        0.1305, 0.1874, 0.0617, 0.1330, 0.2013, 0.0595, 0.0683, 0.0756, 0.1605,\n",
      "        0.1253, 0.1743, 0.2682, 0.0860, 0.0594, 0.1086, 0.0904, 0.0814, 0.1641,\n",
      "        0.1721, 0.2392, 0.1214, 0.0714, 0.2447, 0.0976, 0.2457, 0.1087, 0.1719,\n",
      "        0.1869, 0.1436, 0.1381, 0.1051, 0.0201, 0.1043, 0.1702, 0.1204, 0.2244,\n",
      "        0.0254, 0.2282, 0.2067, 0.2304, 0.1277, 0.0798, 0.1882, 0.0456, 0.0636,\n",
      "        0.1796, 0.1788, 0.0742, 0.2239, 0.0668, 0.1264, 0.1244, 0.1510, 0.2137,\n",
      "        0.2193, 0.1461, 0.1060, 0.1361, 0.1670, 0.1344, 0.0271, 0.0927, 0.1248,\n",
      "        0.0694, 0.0906, 0.2555, 0.1363, 0.1978, 0.0848, 0.1106, 0.1350, 0.1708,\n",
      "        0.1000, 0.2079, 0.1352, 0.1242, 0.0533, 0.1213, 0.1740, 0.1835, 0.1741,\n",
      "        0.0986, 0.0831, 0.2540, 0.1287, 0.0954, 0.1293, 0.1620, 0.1105, 0.1739,\n",
      "        0.0376, 0.1426, 0.1433, 0.0431, 0.1094, 0.1307, 0.2090, 0.1294, 0.0289,\n",
      "        0.1682, 0.0343, 0.1464, 0.1462, 0.2024, 0.1430, 0.1314, 0.1401, 0.2011,\n",
      "        0.1071, 0.1211, 0.1952, 0.0464, 0.0910, 0.1171, 0.1697, 0.0524, 0.1897,\n",
      "        0.3355, 0.1005, 0.2288, 0.0972, 0.0869, 0.1380, 0.0566, 0.2620, 0.1954,\n",
      "        0.1716, 0.1533, 0.1420, 0.0107, 0.1287, 0.1525, 0.2204, 0.2207, 0.0891,\n",
      "        0.1037, 0.1771, 0.2142, 0.1418, 0.0312, 0.0702, 0.1175, 0.0761, 0.1170,\n",
      "        0.0328, 0.1081, 0.0683, 0.1446, 0.0451, 0.1129, 0.0553, 0.0705, 0.1987,\n",
      "        0.0508, 0.0957, 0.0460, 0.2619, 0.1269, 0.1837, 0.1839, 0.1845, 0.2371,\n",
      "        0.1398, 0.1794, 0.0539, 0.0650, 0.1465, 0.0619, 0.1333, 0.0873, 0.1734,\n",
      "        0.1566, 0.1505, 0.0916, 0.1656, 0.1368, 0.1677, 0.0276, 0.1913, 0.2049,\n",
      "        0.0308, 0.1708, 0.2429, 0.2623, 0.2813, 0.1545, 0.1168, 0.1461, 0.2075,\n",
      "        0.0799, 0.1147, 0.0400, 0.1035, 0.1180, 0.1633, 0.1537, 0.1200, 0.1717,\n",
      "        0.2706, 0.4209, 0.0998, 0.1207, 0.2350, 0.1652, 0.0409, 0.1110, 0.1722,\n",
      "        0.0951, 0.1803, 0.1743, 0.2486, 0.0406, 0.1807, 0.1137, 0.0804, 0.1166,\n",
      "        0.1917, 0.1268, 0.1717, 0.1191, 0.2201, 0.1405, 0.0978, 0.1936, 0.1417,\n",
      "        0.1064, 0.1543, 0.0495, 0.1810, 0.1275, 0.1842, 0.1035, 0.1105, 0.2719,\n",
      "        0.1821, 0.1924, 0.0588, 0.0888, 0.1437, 0.1072, 0.2048, 0.1150, 0.0957,\n",
      "        0.1480, 0.0884, 0.0531, 0.1277, 0.0919, 0.0647, 0.0655, 0.1052, 0.1447,\n",
      "        0.1005, 0.1603, 0.2562, 0.2281, 0.1489, 0.1610, 0.1050, 0.1189, 0.1455,\n",
      "        0.1785, 0.1444, 0.0830, 0.0780, 0.1553, 0.2258, 0.1482, 0.0774, 0.0816,\n",
      "        0.1518, 0.1394, 0.2101, 0.0954, 0.0565, 0.0492, 0.0724, 0.0761, 0.1977,\n",
      "        0.1442, 0.2112, 0.1136, 0.1851, 0.0574, 0.0536, 0.1414, 0.1212, 0.2656,\n",
      "        0.2660, 0.0159, 0.0445, 0.1423, 0.1562, 0.0564, 0.1808, 0.0959, 0.0896,\n",
      "        0.0668, 0.2224, 0.3212, 0.1642, 0.0312, 0.2125, 0.1605, 0.1517, 0.1958,\n",
      "        0.1821, 0.1219, 0.1249, 0.0405, 0.1184, 0.2918, 0.0404, 0.1407, 0.2410,\n",
      "        0.0806, 0.1466, 0.1156, 0.1244, 0.1877, 0.2741, 0.0748, 0.1241, 0.0432,\n",
      "        0.1582, 0.2561, 0.0959, 0.1103, 0.0810, 0.0582, 0.2491, 0.1148, 0.1117,\n",
      "        0.1361, 0.1885, 0.2148, 0.1647, 0.2224, 0.1658, 0.2454, 0.1176, 0.0898,\n",
      "        0.1799, 0.1290, 0.2485, 0.0907, 0.1070, 0.1252, 0.1517, 0.0472, 0.1069,\n",
      "        0.0661, 0.0722, 0.2405, 0.0604, 0.1233, 0.2106, 0.1548, 0.1021, 0.1810,\n",
      "        0.0815, 0.1472, 0.0099, 0.1615, 0.2151, 0.0715, 0.1138, 0.0936, 0.0216,\n",
      "        0.0666, 0.1704, 0.0571, 0.3359, 0.0348, 0.1383, 0.0914, 0.1697, 0.0908,\n",
      "        0.1499, 0.2734, 0.0871, 0.0428, 0.1067, 0.0988, 0.0656, 0.1807, 0.2013,\n",
      "        0.0995, 0.0937, 0.1846, 0.0956, 0.1061, 0.1991, 0.0627, 0.0786, 0.1791,\n",
      "        0.0727, 0.1698, 0.0845, 0.2114, 0.1521, 0.0369, 0.2048, 0.1572, 0.0944,\n",
      "        0.1120, 0.0639, 0.1108, 0.2242, 0.2064, 0.0661, 0.1644, 0.0662, 0.1524,\n",
      "        0.1708, 0.1198, 0.1618, 0.1791, 0.0071, 0.2114, 0.2224, 0.1469, 0.1818,\n",
      "        0.1081, 0.1115, 0.2699, 0.0829, 0.1405, 0.1875, 0.0638, 0.1203, 0.2072,\n",
      "        0.2801, 0.1948, 0.1237, 0.1248, 0.1994, 0.1211, 0.0512, 0.0697, 0.1182,\n",
      "        0.1397, 0.1356, 0.0447, 0.1398, 0.1388, 0.0470, 0.3112, 0.0151, 0.2868,\n",
      "        0.1134, 0.0735, 0.1719, 0.1164, 0.0634, 0.0859, 0.1320, 0.1905, 0.0819,\n",
      "        0.1103])\n"
     ]
    }
   ],
   "source": [
    "reshaped_test = test_pearson_list.reshape(-1, 748)\n",
    "cell_type_averages = torch.mean(reshaped_test, dim=1)\n",
    "cell_type_std = torch.std(reshaped_test, dim=1)\n",
    "sequence_averages = torch.mean(reshaped_test, dim=0)\n",
    "sequence_std = torch.std(reshaped_test, dim=0)\n",
    "print(cell_type_averages)\n",
    "print(cell_type_std)\n",
    "print(sequence_averages)\n",
    "print(sequence_std)\n",
    "# with open('try/cell_type_averages_hic_1d.pkl', 'wb') as f:\n",
    "#     pickle.dump(cell_type_averages, f)\n",
    "# with open('try/cell_type_std_hic_1d.pkl', 'wb') as f:\n",
    "#     pickle.dump(cell_type_std, f)\n",
    "# with open('try/sequence_averages_hic_1d.pkl', 'wb') as f:\n",
    "#     pickle.dump(sequence_averages, f)\n",
    "# with open('try/sequence_std_hic_1d.pkl', 'wb') as f:\n",
    "#     pickle.dump(sequence_averages, f)\n",
    "\n",
    "with open('try/cell_type_averages.pkl', 'wb') as f:\n",
    "    pickle.dump(cell_type_averages, f)\n",
    "with open('try/cell_type_std.pkl', 'wb') as f:\n",
    "    pickle.dump(cell_type_std, f)\n",
    "with open('try/sequence_averages.pkl', 'wb') as f:\n",
    "    pickle.dump(sequence_averages, f)\n",
    "with open('try/sequence_std.pkl', 'wb') as f:\n",
    "    pickle.dump(sequence_averages, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.3942), tensor(0.3007), tensor(0.3602), tensor(0.3722), tensor(0.4762), tensor(0.3284)]\n",
      "[tensor(0.2332), tensor(0.2178), tensor(0.2214), tensor(0.2245), tensor(0.2686), tensor(0.2068)]\n",
      "[tensor(0.2767), tensor(0.4628), tensor(0.1289), tensor(0.4690), tensor(0.3716), tensor(0.5032), tensor(0.0442), tensor(0.0150), tensor(0.2543), tensor(0.3910), tensor(0.4852), tensor(0.6636), tensor(0.5059), tensor(0.4629), tensor(0.3956), tensor(0.2309), tensor(0.2599), tensor(0.5445), tensor(0.6472), tensor(0.4737), tensor(0.0079), tensor(0.4112), tensor(0.3693), tensor(0.1962), tensor(0.3377), tensor(0.0894), tensor(0.0178), tensor(0.3988), tensor(-0.0289), tensor(0.1134), tensor(0.5594), tensor(0.4766), tensor(0.2225), tensor(0.3045), tensor(0.5460), tensor(0.5238), tensor(0.5216), tensor(0.2024), tensor(-0.0001), tensor(0.4312), tensor(0.5544), tensor(0.4389), tensor(0.5341), tensor(0.5085), tensor(0.5247), tensor(0.4614), tensor(0.0356), tensor(0.2215), tensor(0.3240), tensor(0.0087), tensor(0.5453), tensor(0.6965), tensor(0.1400), tensor(0.7069), tensor(0.3829), tensor(0.4474), tensor(0.6759), tensor(0.7650), tensor(0.5247), tensor(0.4753), tensor(0.5397), tensor(0.2873), tensor(0.3158), tensor(0.1661), tensor(0.5841), tensor(0.1321), tensor(0.5344), tensor(0.1521), tensor(0.5998), tensor(0.5445), tensor(0.2006), tensor(0.7879), tensor(0.2223), tensor(0.5528), tensor(0.4437), tensor(0.2405), tensor(0.5223), tensor(0.4080), tensor(0.1873), tensor(0.3869), tensor(0.6265), tensor(0.2105), tensor(0.4546), tensor(0.2025), tensor(0.6069), tensor(0.5560), tensor(0.3907), tensor(0.3779), tensor(0.5102), tensor(0.6426), tensor(0.3437), tensor(0.3626), tensor(-0.0403), tensor(0.5356), tensor(0.4838), tensor(0.4614), tensor(0.6262), tensor(0.4623), tensor(0.5083), tensor(0.1035), tensor(0.4521), tensor(0.4635), tensor(0.5548), tensor(0.5515), tensor(0.4757), tensor(0.4532), tensor(0.2384), tensor(0.4253), tensor(0.3253), tensor(0.1666), tensor(0.3629), tensor(0.7369), tensor(0.1748), tensor(0.5462), tensor(0.3999), tensor(0.4079), tensor(0.4608), tensor(0.4851), tensor(0.5869), tensor(0.3191), tensor(0.5499), tensor(0.5829), tensor(0.6578), tensor(0.5741), tensor(0.2362), tensor(0.5325), tensor(0.3861), tensor(0.4469), tensor(0.4697), tensor(0.1651), tensor(0.1623), tensor(0.5302), tensor(0.2055), tensor(0.1164), tensor(0.1020), tensor(0.7462), tensor(0.5451), tensor(0.4180), tensor(0.4089), tensor(0.5923), tensor(0.4147), tensor(0.2584), tensor(0.6499), tensor(0.3552), tensor(0.1061), tensor(0.6019), tensor(0.1915), tensor(0.2963), tensor(0.5394), tensor(0.4104), tensor(0.0972), tensor(0.0321), tensor(0.3665), tensor(0.2245), tensor(0.1511), tensor(0.6975), tensor(0.2806), tensor(0.0373), tensor(0.4802), tensor(-0.0237), tensor(0.5404), tensor(0.3589), tensor(0.3967), tensor(0.3336), tensor(0.2861), tensor(0.1832), tensor(0.5602), tensor(0.1909), tensor(0.3858), tensor(0.3698), tensor(0.4274), tensor(0.3568), tensor(0.4726), tensor(0.4925), tensor(0.5613), tensor(0.8392), tensor(0.0223), tensor(0.4332), tensor(0.5313), tensor(0.4563), tensor(0.2318), tensor(0.5354), tensor(0.6142), tensor(0.4205), tensor(0.5081), tensor(0.3633), tensor(0.3333), tensor(0.2557), tensor(0.4049), tensor(0.2522), tensor(0.6795), tensor(0.2516), tensor(0.1491), tensor(0.2894), tensor(0.5755), tensor(0.1698), tensor(0.2807), tensor(0.4555), tensor(0.6621), tensor(0.1985), tensor(0.1585), tensor(0.1720), tensor(0.5150), tensor(0.5576), tensor(0.6848), tensor(0.4169), tensor(0.4653), tensor(0.0502), tensor(0.4847), tensor(0.3496), tensor(0.4179), tensor(0.1860), tensor(0.2409), tensor(0.4024), tensor(0.4175), tensor(0.3068), tensor(0.3080), tensor(0.4927), tensor(0.3710), tensor(0.5607), tensor(0.3901), tensor(0.3783), tensor(0.4079), tensor(0.3210), tensor(0.0995), tensor(0.5602), tensor(0.3624), tensor(0.4531), tensor(0.5094), tensor(0.0937), tensor(0.4968), tensor(0.4902), tensor(0.4893), tensor(0.2666), tensor(0.2846), tensor(0.1142), tensor(0.5928), tensor(0.4298), tensor(0.4923), tensor(0.3883), tensor(0.0609), tensor(0.4223), tensor(0.1481), tensor(0.3466), tensor(0.3616), tensor(0.2293), tensor(0.5298), tensor(0.4243), tensor(0.3377), tensor(0.2119), tensor(0.3120), tensor(0.4097), tensor(0.4125), tensor(0.2927), tensor(0.4606), tensor(0.3689), tensor(0.3367), tensor(0.5907), tensor(0.1932), tensor(0.6020), tensor(0.6144), tensor(0.2583), tensor(0.4599), tensor(0.4159), tensor(0.0081), tensor(0.7194), tensor(0.3267), tensor(0.4791), tensor(0.4549), tensor(0.4648), tensor(0.6617), tensor(0.4702), tensor(0.4986), tensor(0.1801), tensor(0.1446), tensor(0.3092), tensor(-0.0203), tensor(0.4715), tensor(0.4824), tensor(0.3140), tensor(0.4727), tensor(0.4729), tensor(0.4814), tensor(0.6950), tensor(0.2726), tensor(0.5347), tensor(0.2368), tensor(0.4405), tensor(0.2550), tensor(0.2903), tensor(0.6670), tensor(0.5267), tensor(0.5289), tensor(0.4460), tensor(0.2763), tensor(0.4125), tensor(0.2865), tensor(0.5409), tensor(0.3403), tensor(0.4706), tensor(0.2529), tensor(0.5655), tensor(0.3630), tensor(0.0983), tensor(0.5646), tensor(0.3971), tensor(0.2291), tensor(0.3343), tensor(0.6476), tensor(0.8875), tensor(0.5095), tensor(0.5926), tensor(0.3535), tensor(0.5944), tensor(0.2863), tensor(0.6221), tensor(0.6252), tensor(0.3900), tensor(0.5334), tensor(-0.0059), tensor(0.6175), tensor(0.4049), tensor(0.5161), tensor(0.3472), tensor(0.0482), tensor(0.4769), tensor(0.4752), tensor(0.2963), tensor(0.6048), tensor(0.5320), tensor(0.3608), tensor(0.6078), tensor(0.3733), tensor(0.3981), tensor(0.2564), tensor(0.2849), tensor(0.3019), tensor(0.4780), tensor(0.1526), tensor(0.1856), tensor(0.4164), tensor(0.3593), tensor(0.4867), tensor(0.6566), tensor(0.5400), tensor(0.4755), tensor(0.1887), tensor(0.3149), tensor(-0.0370), tensor(0.1276), tensor(0.4450), tensor(0.0776), tensor(0.3691), tensor(0.4855), tensor(0.4559), tensor(0.4054), tensor(0.1092), tensor(0.1735), tensor(0.1841), tensor(0.4756), tensor(0.5396), tensor(0.5058), tensor(0.2420), tensor(0.5620), tensor(-0.0158), tensor(0.3457), tensor(0.3291), tensor(0.6125), tensor(0.5358), tensor(0.2085), tensor(0.0757), tensor(0.3016), tensor(0.1976), tensor(0.5869), tensor(0.7425), tensor(0.4823), tensor(0.6448), tensor(0.3110), tensor(-6.6720e-06), tensor(0.2334), tensor(0.0884), tensor(0.0101), tensor(0.4656), tensor(0.1266), tensor(0.1844), tensor(0.5982), tensor(0.0148), tensor(0.3258), tensor(0.0097), tensor(0.4117), tensor(0.5150), tensor(0.5179), tensor(0.2796), tensor(0.4030), tensor(0.5405), tensor(0.6297), tensor(0.1871), tensor(0.5723), tensor(0.2382), tensor(-0.0152), tensor(0.5232), tensor(0.0726), tensor(0.5847), tensor(0.1929), tensor(0.4067), tensor(0.2256), tensor(0.0318), tensor(0.5944), tensor(0.4887), tensor(0.5251), tensor(0.5742), tensor(0.6416), tensor(0.4945), tensor(0.1628), tensor(0.4747), tensor(0.1726), tensor(0.5782), tensor(-0.0195), tensor(0.2901), tensor(0.5171), tensor(0.4884), tensor(0.5338), tensor(0.6943), tensor(0.1128), tensor(0.2257), tensor(0.3346), tensor(0.5708), tensor(-0.0191), tensor(0.0346), tensor(0.4672), tensor(0.0443), tensor(0.4793), tensor(-0.0329), tensor(0.5918), tensor(0.0901), tensor(0.4907), tensor(-0.0313), tensor(0.1584), tensor(-0.0541), tensor(0.4541), tensor(0.2587), tensor(0.0467), tensor(0.5388), tensor(0.1086), tensor(0.2533), tensor(0.4510), tensor(0.4813), tensor(0.4594), tensor(0.1278), tensor(0.2187), tensor(0.2353), tensor(0.3769), tensor(0.0391), tensor(0.0808), tensor(0.2073), tensor(0.1305), tensor(0.5860), tensor(0.0277), tensor(0.3635), tensor(0.4968), tensor(0.3303), tensor(0.1432), tensor(0.5261), tensor(0.2698), tensor(0.2936), tensor(-0.0471), tensor(0.2717), tensor(0.4840), tensor(-0.0265), tensor(0.4603), tensor(0.2275), tensor(0.5794), tensor(0.4148), tensor(0.1826), tensor(0.5949), tensor(0.3763), tensor(0.3936), tensor(0.0068), tensor(0.5404), tensor(0.9357), tensor(0.4465), tensor(0.3319), tensor(0.5968), tensor(0.4975), tensor(0.4394), tensor(0.3965), tensor(0.5922), tensor(0.3843), tensor(0.6494), tensor(0.3273), tensor(0.1453), tensor(0.6544), tensor(0.0180), tensor(0.6878), tensor(0.2360), tensor(0.1936), tensor(0.4185), tensor(0.4354), tensor(0.5224), tensor(0.0497), tensor(0.4236), tensor(0.2128), tensor(0.1170), tensor(0.4364), tensor(0.6398), tensor(0.6742), tensor(0.4808), tensor(0.1029), tensor(0.2590), tensor(0.3203), tensor(0.7230), tensor(0.6805), tensor(0.6555), tensor(0.1508), tensor(0.5131), tensor(-0.0100), tensor(0.2946), tensor(0.5025), tensor(0.2286), tensor(0.5547), tensor(0.5312), tensor(0.3959), tensor(0.5261), tensor(0.5261), tensor(0.0639), tensor(0.6465), tensor(0.5588), tensor(0.3307), tensor(0.1939), tensor(0.2771), tensor(0.1593), tensor(0.2641), tensor(0.5871), tensor(0.2416), tensor(0.2184), tensor(0.1197), tensor(0.5386), tensor(0.0853), tensor(0.5445), tensor(0.4740), tensor(0.3453), tensor(0.1928), tensor(0.4050), tensor(0.4277), tensor(0.6104), tensor(0.1883), tensor(0.2508), tensor(0.2238), tensor(0.2962), tensor(0.2242), tensor(0.0751), tensor(0.5104), tensor(0.4377), tensor(0.1134), tensor(0.5649), tensor(0.2321), tensor(0.7427), tensor(0.1305), tensor(0.6251), tensor(0.5021), tensor(0.4114), tensor(0.3553), tensor(0.0294), tensor(0.0381), tensor(0.7038), tensor(0.0739), tensor(0.1155), tensor(0.1594), tensor(0.2280), tensor(0.3823), tensor(0.2071), tensor(0.0181), tensor(-0.0695), tensor(0.1159), tensor(0.1253), tensor(0.2777), tensor(0.4830), tensor(0.0221), tensor(0.4087), tensor(0.2799), tensor(0.2579), tensor(0.0360), tensor(0.6481), tensor(0.3657), tensor(0.5412), tensor(0.4641), tensor(0.4233), tensor(0.4130), tensor(0.3402), tensor(-0.0133), tensor(0.3916), tensor(0.3688), tensor(0.4997), tensor(0.2424), tensor(0.2976), tensor(0.1936), tensor(0.4699), tensor(0.0410), tensor(0.3439), tensor(0.3432), tensor(0.0192), tensor(0.4184), tensor(0.3251), tensor(0.6607), tensor(0.3746), tensor(0.6205), tensor(0.0801), tensor(0.4474), tensor(0.5130), tensor(0.0592), tensor(0.2125), tensor(-0.0543), tensor(0.5850), tensor(0.4341), tensor(0.6165), tensor(0.3662), tensor(0.4354), tensor(0.5466), tensor(0.4350), tensor(0.4062), tensor(0.5525), tensor(0.1678), tensor(0.4673), tensor(0.1767), tensor(0.4955), tensor(0.3885), tensor(0.2139), tensor(0.3304), tensor(0.1705), tensor(0.0528), tensor(0.1426), tensor(0.6022), tensor(0.3866), tensor(0.0476), tensor(0.4051), tensor(0.4343), tensor(0.5543), tensor(0.0411), tensor(0.6118), tensor(0.3482), tensor(0.0527), tensor(0.3674), tensor(0.7181), tensor(0.2504), tensor(0.5547), tensor(0.3562), tensor(0.1216), tensor(0.5177), tensor(0.1069), tensor(0.4321), tensor(-0.0083), tensor(0.5197), tensor(0.2392), tensor(0.7515), tensor(-0.0218), tensor(0.1113), tensor(-0.0074), tensor(0.0473), tensor(0.2338), tensor(0.1131), tensor(0.1966), tensor(-0.0542), tensor(0.4283), tensor(0.4792), tensor(0.4988), tensor(0.0293), tensor(0.4074), tensor(0.1929), tensor(0.0395), tensor(0.0255), tensor(0.3665), tensor(0.2845), tensor(0.0602), tensor(0.2066), tensor(0.4662), tensor(0.3096), tensor(0.4430), tensor(0.1479), tensor(0.4204), tensor(0.6422), tensor(0.5040), tensor(0.1137), tensor(0.0515), tensor(0.2209), tensor(0.1448), tensor(0.5873), tensor(0.0302), tensor(0.2687), tensor(0.2034), tensor(0.0591), tensor(0.3532), tensor(0.2014), tensor(0.5574), tensor(0.1068), tensor(0.0230), tensor(0.3158), tensor(0.5559), tensor(0.1405), tensor(0.6324), tensor(0.4174), tensor(0.0957), tensor(0.0476), tensor(0.3366), tensor(0.0201), tensor(0.4352), tensor(0.5945), tensor(-0.0137), tensor(0.4590), tensor(0.1576), tensor(0.7251), tensor(0.3747), tensor(0.7257), tensor(0.0456), tensor(0.3833), tensor(0.7083), tensor(0.4982), tensor(0.2933), tensor(0.6834), tensor(0.6893), tensor(0.4130), tensor(0.5074), tensor(0.2774), tensor(0.0159), tensor(0.4708), tensor(0.2172), tensor(0.5678), tensor(0.0645), tensor(0.0827), tensor(0.2605), tensor(0.5515), tensor(0.5201), tensor(0.0389), tensor(0.6095), tensor(0.0845), tensor(-0.0472), tensor(0.3374), tensor(-0.0032), tensor(0.4255), tensor(0.5283), tensor(0.3180), tensor(0.5897), tensor(0.5999), tensor(0.0886), tensor(0.0470), tensor(0.4022), tensor(0.7083), tensor(0.3550), tensor(0.5383)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.1151), tensor(0.1623), tensor(0.1416), tensor(0.1455), tensor(0.1422), tensor(0.1386), tensor(0.0678), tensor(nan), tensor(0.1202), tensor(0.1540), tensor(0.1597), tensor(0.1547), tensor(0.1074), tensor(0.2515), tensor(0.0856), tensor(0.1601), tensor(0.0835), tensor(0.1026), tensor(0.0965), tensor(0.1661), tensor(0.0503), tensor(0.0948), tensor(0.0972), tensor(0.1349), tensor(0.2415), tensor(0.0556), tensor(0.0339), tensor(0.1638), tensor(0.0284), tensor(0.1654), tensor(0.0856), tensor(0.1051), tensor(0.1306), tensor(0.0732), tensor(0.1336), tensor(0.1384), tensor(0.1240), tensor(0.1893), tensor(0.0414), tensor(0.1470), tensor(0.1588), tensor(0.1933), tensor(0.0811), tensor(0.2134), tensor(0.1527), tensor(0.1785), tensor(0.1626), tensor(0.1310), tensor(0.0880), tensor(0.0973), tensor(0.1772), tensor(0.1136), tensor(0.0357), tensor(0.1526), tensor(0.1192), tensor(0.1332), tensor(0.1110), tensor(0.1151), tensor(0.1659), tensor(0.2005), tensor(0.1145), tensor(0.2240), tensor(0.1791), tensor(0.1177), tensor(0.1175), tensor(0.1951), tensor(0.1009), tensor(0.1170), tensor(0.1230), tensor(0.1826), tensor(0.1481), tensor(0.1347), tensor(0.1494), tensor(0.1476), tensor(0.1630), tensor(0.1408), tensor(0.0744), tensor(0.1086), tensor(0.1645), tensor(0.2416), tensor(0.1688), tensor(0.1517), tensor(0.0804), tensor(0.1140), tensor(0.0651), tensor(0.0948), tensor(0.0706), tensor(0.1274), tensor(0.1549), tensor(0.1267), tensor(0.1348), tensor(0.1102), tensor(0.1190), tensor(0.1076), tensor(0.2257), tensor(0.1416), tensor(0.1741), tensor(0.1219), tensor(0.1210), tensor(0.1168), tensor(0.1626), tensor(0.1131), tensor(0.1987), tensor(0.1123), tensor(0.1419), tensor(0.0845), tensor(0.1319), tensor(0.1172), tensor(0.1900), tensor(0.0849), tensor(0.1221), tensor(0.1185), tensor(0.1222), tensor(0.1608), tensor(0.0983), tensor(0.1297), tensor(0.1242), tensor(0.0429), tensor(0.1819), tensor(0.1840), tensor(0.1267), tensor(0.1660), tensor(0.0787), tensor(0.1333), tensor(0.1340), tensor(0.1449), tensor(0.1466), tensor(0.1282), tensor(0.2104), tensor(0.1660), tensor(0.1220), tensor(0.0765), tensor(0.0992), tensor(0.1530), tensor(0.0905), tensor(0.0781), tensor(0.1736), tensor(0.1034), tensor(0.2214), tensor(0.0761), tensor(0.2069), tensor(0.0212), tensor(0.1283), tensor(0.0759), tensor(0.1672), tensor(0.1729), tensor(0.1138), tensor(0.1002), tensor(0.2107), tensor(0.2131), tensor(0.0214), tensor(0.0867), tensor(0.1361), tensor(0.2237), tensor(0.1213), tensor(0.1353), tensor(0.1312), tensor(0.0407), tensor(0.1341), tensor(0.0905), tensor(0.1369), tensor(0.1491), tensor(0.0915), tensor(0.1667), tensor(0.2326), tensor(0.2050), tensor(0.0880), tensor(0.1413), tensor(0.1863), tensor(0.1124), tensor(0.0911), tensor(0.0979), tensor(0.1312), tensor(0.0968), tensor(0.2320), tensor(0.0473), tensor(0.0658), tensor(0.1279), tensor(0.1539), tensor(0.1779), tensor(0.2961), tensor(0.0970), tensor(0.1034), tensor(0.1371), tensor(0.1439), tensor(0.1395), tensor(0.1376), tensor(0.1260), tensor(0.1739), tensor(0.1108), tensor(0.0727), tensor(0.1864), tensor(0.1511), tensor(0.2394), tensor(0.1140), tensor(0.1722), tensor(0.1062), tensor(0.1938), tensor(0.0922), tensor(0.1214), tensor(0.1607), tensor(0.0794), tensor(0.2060), tensor(0.1581), tensor(0.1366), tensor(0.0853), tensor(0.1077), tensor(0.0798), tensor(0.1197), tensor(0.1220), tensor(0.1839), tensor(0.1229), tensor(0.2462), tensor(0.1585), tensor(0.1598), tensor(0.0297), tensor(0.1068), tensor(0.1571), tensor(0.1494), tensor(0.2657), tensor(0.2292), tensor(0.1211), tensor(0.1015), tensor(0.2525), tensor(0.1278), tensor(0.2732), tensor(0.1579), tensor(0.0499), tensor(0.1698), tensor(0.0938), tensor(0.1275), tensor(0.1747), tensor(0.1335), tensor(0.1914), tensor(0.1977), tensor(0.1001), tensor(0.0873), tensor(0.0822), tensor(0.2664), tensor(0.1549), tensor(0.3312), tensor(0.2684), tensor(0.0903), tensor(0.1449), tensor(0.0984), tensor(0.1160), tensor(0.1763), tensor(0.1447), tensor(0.0790), tensor(0.1107), tensor(0.2011), tensor(0.1318), tensor(0.1416), tensor(0.1001), tensor(0.1157), tensor(0.1009), tensor(0.1393), tensor(0.0750), tensor(0.1235), tensor(0.1295), tensor(0.0818), tensor(0.0573), tensor(0.0599), tensor(0.1636), tensor(0.0379), tensor(0.0881), tensor(0.0723), tensor(0.1432), tensor(0.1169), tensor(0.1480), tensor(0.1086), tensor(0.1481), tensor(0.1856), tensor(0.1423), tensor(0.1161), tensor(0.0771), tensor(0.0304), tensor(0.1011), tensor(0.1097), tensor(0.1060), tensor(0.1585), tensor(0.1215), tensor(0.2143), tensor(0.0802), tensor(0.1077), tensor(0.1257), tensor(0.2738), tensor(0.1537), tensor(0.1305), tensor(0.1874), tensor(0.0617), tensor(0.1330), tensor(0.2013), tensor(0.0595), tensor(0.0683), tensor(0.0756), tensor(0.1605), tensor(0.1253), tensor(0.1743), tensor(0.2682), tensor(0.0860), tensor(0.0594), tensor(0.1086), tensor(0.0965), tensor(0.0814), tensor(0.1641), tensor(0.1721), tensor(0.2196), tensor(0.1214), tensor(0.0714), tensor(0.2447), tensor(0.0976), tensor(0.2222), tensor(0.1087), tensor(0.1719), tensor(0.1869), tensor(0.1436), tensor(0.1381), tensor(0.1051), tensor(0.0201), tensor(0.1043), tensor(0.1702), tensor(0.1204), tensor(0.1944), tensor(0.0107), tensor(0.2282), tensor(0.2067), tensor(0.2193), tensor(0.1277), tensor(0.0798), tensor(0.1882), tensor(0.0456), tensor(0.0636), tensor(0.1796), tensor(0.1788), tensor(0.0742), tensor(0.2239), tensor(0.0668), tensor(0.1264), tensor(0.1244), tensor(0.1510), tensor(0.2137), tensor(0.2193), tensor(0.1461), tensor(0.1060), tensor(0.1361), tensor(0.1670), tensor(0.1344), tensor(0.0249), tensor(0.0963), tensor(0.1248), tensor(0.0690), tensor(0.0906), tensor(0.2555), tensor(0.1363), tensor(0.1978), tensor(0.0817), tensor(0.1106), tensor(0.1350), tensor(0.1708), tensor(0.1000), tensor(0.2079), tensor(0.1352), tensor(0.1242), tensor(0.0591), tensor(0.1213), tensor(0.1740), tensor(0.1835), tensor(0.1741), tensor(0.0986), tensor(0.0947), tensor(0.2540), tensor(0.1287), tensor(0.0954), tensor(0.1293), tensor(0.1620), tensor(0.1105), tensor(0.1739), tensor(0.0420), tensor(0.1426), tensor(0.1551), tensor(0.0431), tensor(0.1094), tensor(0.1461), tensor(0.2090), tensor(0.1294), tensor(0.0289), tensor(0.1682), tensor(0.0381), tensor(0.1464), tensor(0.1462), tensor(0.2024), tensor(0.1430), tensor(0.1314), tensor(0.1401), tensor(0.2011), tensor(0.0595), tensor(0.1211), tensor(0.1952), tensor(0.0514), tensor(0.0910), tensor(0.1267), tensor(0.1697), tensor(0.0524), tensor(0.1897), tensor(0.3355), tensor(0.1005), tensor(0.2288), tensor(0.0972), tensor(0.0869), tensor(0.1380), tensor(0.0566), tensor(0.2620), tensor(0.1954), tensor(0.1716), tensor(0.1611), tensor(0.1420), tensor(0.0018), tensor(0.1287), tensor(0.1525), tensor(0.2204), tensor(0.2207), tensor(0.0891), tensor(0.1316), tensor(0.2983), tensor(0.2142), tensor(0.1418), tensor(0.0382), tensor(0.0769), tensor(0.1175), tensor(0.0826), tensor(0.1170), tensor(0.0628), tensor(0.1081), tensor(0.0683), tensor(0.1446), tensor(0.0451), tensor(0.1004), tensor(0.0553), tensor(0.0705), tensor(0.1987), tensor(0.0508), tensor(0.0957), tensor(0.0460), tensor(0.2930), tensor(0.1269), tensor(0.1837), tensor(0.1839), tensor(0.1845), tensor(0.2371), tensor(0.0859), tensor(0.1794), tensor(0.0781), tensor(0.0650), tensor(0.1465), tensor(0.0619), tensor(0.1333), tensor(0.1111), tensor(0.1734), tensor(0.1566), tensor(0.1505), tensor(0.0788), tensor(0.1656), tensor(0.0907), tensor(0.1677), tensor(0.0222), tensor(0.1742), tensor(0.2049), tensor(0.0308), tensor(0.1708), tensor(0.2429), tensor(0.2623), tensor(0.2813), tensor(0.1513), tensor(0.1168), tensor(0.1461), tensor(0.2075), tensor(0.0799), tensor(0.1147), tensor(0.0400), tensor(0.1035), tensor(0.1180), tensor(0.1633), tensor(0.1537), tensor(0.1200), tensor(0.1717), tensor(0.2706), tensor(0.4792), tensor(0.0998), tensor(0.1207), tensor(0.2350), tensor(0.1652), tensor(0.0514), tensor(0.1110), tensor(0.1722), tensor(0.0951), tensor(0.1803), tensor(0.1743), tensor(0.1428), tensor(0.0477), tensor(0.1807), tensor(0.1137), tensor(0.0724), tensor(0.1166), tensor(0.1917), tensor(0.1268), tensor(0.1717), tensor(0.1246), tensor(0.2201), tensor(0.1405), tensor(0.0978), tensor(0.1936), tensor(0.1417), tensor(0.0970), tensor(0.1543), tensor(0.0635), tensor(0.1810), tensor(0.1275), tensor(0.1842), tensor(0.1035), tensor(0.1105), tensor(0.2593), tensor(0.1821), tensor(0.1924), tensor(0.0748), tensor(0.0888), tensor(0.1437), tensor(0.1072), tensor(0.2769), tensor(0.1150), tensor(0.0957), tensor(0.1480), tensor(0.0884), tensor(0.0531), tensor(0.1277), tensor(0.0871), tensor(0.0647), tensor(0.1083), tensor(0.1052), tensor(0.1447), tensor(0.1005), tensor(0.1603), tensor(0.2562), tensor(0.2281), tensor(0.1489), tensor(0.1657), tensor(0.1050), tensor(0.1189), tensor(0.1455), tensor(0.1785), tensor(0.1444), tensor(0.0830), tensor(0.0780), tensor(0.1553), tensor(0.2258), tensor(0.1125), tensor(0.0774), tensor(0.0623), tensor(0.1518), tensor(0.1394), tensor(0.2101), tensor(0.0954), tensor(0.0856), tensor(0.0492), tensor(0.0724), tensor(0.1019), tensor(0.2146), tensor(0.1442), tensor(0.2120), tensor(0.1136), tensor(0.1951), tensor(0.0893), tensor(0.0890), tensor(0.1414), tensor(0.1212), tensor(0.2656), tensor(0.2660), tensor(0.0143), tensor(0.0445), tensor(0.1423), tensor(0.1290), tensor(0.0687), tensor(0.1808), tensor(0.0959), tensor(0.0896), tensor(0.0668), tensor(0.2224), tensor(0.3212), tensor(0.1642), tensor(0.0343), tensor(0.2125), tensor(0.1605), tensor(0.1517), tensor(0.1944), tensor(0.1821), tensor(0.1219), tensor(0.1249), tensor(0.0412), tensor(0.1184), tensor(0.2918), tensor(0.0616), tensor(0.1407), tensor(0.2410), tensor(0.0806), tensor(0.1466), tensor(0.1156), tensor(0.1342), tensor(0.1877), tensor(0.2741), tensor(0.0792), tensor(0.1241), tensor(0.0734), tensor(0.1582), tensor(0.2561), tensor(0.0959), tensor(0.1103), tensor(0.0810), tensor(0.0582), tensor(0.2491), tensor(0.1148), tensor(0.1117), tensor(0.1361), tensor(0.1885), tensor(0.3033), tensor(0.1647), tensor(0.2224), tensor(0.1658), tensor(0.2454), tensor(0.1176), tensor(0.0975), tensor(0.1799), tensor(0.1290), tensor(0.2145), tensor(0.0907), tensor(0.1070), tensor(0.1252), tensor(0.1517), tensor(0.0493), tensor(0.1069), tensor(0.0661), tensor(0.0863), tensor(0.2405), tensor(0.0604), tensor(0.1233), tensor(0.2106), tensor(0.1548), tensor(0.0997), tensor(0.1810), tensor(0.1342), tensor(0.1472), tensor(0.0201), tensor(0.1615), tensor(0.2151), tensor(0.0715), tensor(0.1268), tensor(0.0915), tensor(0.0476), tensor(0.0666), tensor(0.1704), tensor(0.0571), tensor(0.3646), tensor(0.0266), tensor(0.1383), tensor(0.0914), tensor(0.1697), tensor(0.0908), tensor(0.1499), tensor(0.3286), tensor(0.0956), tensor(0.0428), tensor(0.1067), tensor(0.0988), tensor(0.0746), tensor(0.2228), tensor(0.2013), tensor(0.0995), tensor(0.0937), tensor(0.1846), tensor(0.0956), tensor(0.1061), tensor(0.1991), tensor(0.0471), tensor(0.1160), tensor(0.1791), tensor(0.0727), tensor(0.1698), tensor(0.0935), tensor(0.2114), tensor(0.1521), tensor(0.0369), tensor(0.2048), tensor(0.1572), tensor(0.0944), tensor(0.1154), tensor(0.0706), tensor(0.1108), tensor(0.2242), tensor(0.2216), tensor(0.0661), tensor(0.1644), tensor(0.0568), tensor(0.1690), tensor(0.1708), tensor(0.1198), tensor(0.1618), tensor(0.1791), tensor(0.0015), tensor(0.2114), tensor(0.2380), tensor(0.1469), tensor(0.1818), tensor(0.1081), tensor(0.1115), tensor(0.2699), tensor(0.0829), tensor(0.1405), tensor(0.1875), tensor(0.0638), tensor(0.1203), tensor(0.1346), tensor(0.2801), tensor(0.1948), tensor(0.1381), tensor(0.1248), tensor(0.1996), tensor(0.1211), tensor(0.0585), tensor(0.0710), tensor(0.1182), tensor(0.1397), tensor(0.1356), tensor(0.0621), tensor(0.1398), tensor(0.1503), tensor(0.0480), tensor(0.3112), tensor(0.0193), tensor(0.2868), tensor(0.1134), tensor(0.0735), tensor(0.1719), tensor(0.1164), tensor(0.0582), tensor(0.0859), tensor(0.1320), tensor(0.1905), tensor(0.0819), tensor(0.1103)]\n"
     ]
    }
   ],
   "source": [
    "cell_type_averages = []\n",
    "cell_type_std = []\n",
    "for subarray in reshaped_test:\n",
    "    non_zero_elements = subarray[subarray != 0]\n",
    "    cell_type_averages.append(torch.mean(non_zero_elements))\n",
    "    cell_type_std.append(torch.std(non_zero_elements))\n",
    "print(cell_type_averages)\n",
    "print(cell_type_std)\n",
    "\n",
    "sequence_averages = []\n",
    "sequence_std = []\n",
    "for i in range(reshaped_test.size(1)):\n",
    "    subarray = reshaped_test[:, i]\n",
    "    non_zero_elements = subarray[subarray != 0]\n",
    "    sequence_averages.append(torch.mean(non_zero_elements))\n",
    "    sequence_std.append(torch.std(non_zero_elements))\n",
    "print(sequence_averages)\n",
    "print(sequence_std)\n",
    "# with open('try/cell_type_averages_hic_1d_no0.pkl', 'wb') as f:\n",
    "#     pickle.dump(cell_type_averages, f)\n",
    "# with open('try/cell_type_std_hic_1d_no0.pkl', 'wb') as f:\n",
    "#     pickle.dump(cell_type_std, f)\n",
    "# with open('try/sequence_averages_hic_1d_no0.pkl', 'wb') as f:\n",
    "#     pickle.dump(sequence_averages, f)\n",
    "# with open('try/sequence_std_hic_1d_no0.pkl', 'wb') as f:\n",
    "#     pickle.dump(sequence_averages, f)\n",
    "\n",
    "with open('try/cell_type_averages_no0.pkl', 'wb') as f:\n",
    "    pickle.dump(cell_type_averages, f)\n",
    "with open('try/cell_type_std_no0.pkl', 'wb') as f:\n",
    "    pickle.dump(cell_type_std, f)\n",
    "with open('try/sequence_averages_no0.pkl', 'wb') as f:\n",
    "    pickle.dump(sequence_averages, f)\n",
    "with open('try/sequence_std_no0.pkl', 'wb') as f:\n",
    "    pickle.dump(sequence_averages, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
